{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드 및 전처리 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 PyTorch DataLoader로 불러올 필요가 있습니다. train, test, val 데이터를 각각 분할해 모델 학습 및 평가에 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seoyun/miniconda3/envs/engnews/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Load CSV and prepare dataset\u001b[39;00m\n\u001b[1;32m     42\u001b[0m dataframe     \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn_dailymail/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# 경로에 맞게 수정하세요\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m tokenizer     \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m NewsDataset(dataframe[dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], tokenizer, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     45\u001b[0m val_dataset   \u001b[38;5;241m=\u001b[39m NewsDataset(dataframe[dataframe[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m], tokenizer, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/engnews/lib/python3.10/site-packages/transformers/utils/import_utils.py:1651\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1651\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/engnews/lib/python3.10/site-packages/transformers/utils/import_utils.py:1639\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1637\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/bigbird-pegasus-large-arxiv')  # 저장된 토크나이저 경로\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('google/bigbird-pegasus-large-arxiv')  # 저장된 모델 경로\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        article = self.data.iloc[index]['article']\n",
    "        highlights = self.data.iloc[index]['highlights']\n",
    "\n",
    "        # Tokenize input and target text\n",
    "        input_ids = self.tokenizer.encode(\n",
    "            f\"summarize: {article}\",\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length = self.max_len,\n",
    "            padding= \"max_length\"\n",
    "        ).squeeze()\n",
    "\n",
    "        target_ids = self.tokenizer.encode(\n",
    "            highlights,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            max_length = self.max_len // 2,\n",
    "            padding='max_length'\n",
    "        ).squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'target_ids': target_ids\n",
    "        }\n",
    "\n",
    "# Load CSV files and prepare dataset\n",
    "train_data_path = '/content/drive/MyDrive/cnn_dailymail_dataset/cnn_dailymail/train.csv'\n",
    "val_data_path = '/content/drive/MyDrive/cnn_dailymail_dataset/cnn_dailymail/validation.csv'\n",
    "test_data_path = '/content/drive/MyDrive/cnn_dailymail_dataset/cnn_dailymail/test.csv'\n",
    "\n",
    "# Load dataframes\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "val_df = pd.read_csv(val_data_path)\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = NewsDataset(train_df, tokenizer, max_len=1500)\n",
    "val_dataset = NewsDataset(val_df, tokenizer, max_len=1500)\n",
    "test_dataset = NewsDataset(test_df, tokenizer, max_len=1500)\n",
    "\n",
    "# DataLoader for training, validation, and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 학습 및 성능 평가를 위한 코드입니다. 주어진 데이터셋의 validation 데이터로 중간중간 평가하면서, 가장 좋은 성능을 낸 모델을 저장하도록 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification\n",
    "import torch\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "model.to(device)\n",
    "\n",
    "# Training and evaluation loop\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "save_interval = 800  # Save model after every 500 batches\n",
    "log_interval = 500  # Print loss every 100 batches\n",
    "val_chunk_size = 100  # Number of validation batches to evaluate\n",
    "\n",
    "model.train()\n",
    "step = 0\n",
    "\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    step += 1\n",
    "\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    target_ids = batch['target_ids'].to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, labels=target_ids)\n",
    "    loss = outputs.loss\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print training loss at intervals\n",
    "    if step % log_interval == 0:\n",
    "        print(f\"Step {step} - Training Loss: {loss.item()}\")\n",
    "\n",
    "    # Validation and save model at intervals\n",
    "    if step % save_interval == 0:\n",
    "        print(\"Running validation on a random chunk...\")\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        # Randomly sample a subset of validation data\n",
    "        sampled_indices = random.sample(range(len(val_loader)), val_chunk_size)\n",
    "        sampled_batches = [val_loader.dataset[i] for i in sampled_indices]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_batch in sampled_batches:\n",
    "                val_input_ids = val_batch['input_ids'].unsqueeze(0).to(device)\n",
    "                val_target_ids = val_batch['target_ids'].unsqueeze(0).to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                val_outputs = model(input_ids=val_input_ids, labels=val_target_ids)\n",
    "                val_loss += val_outputs.loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / val_chunk_size\n",
    "        print(f\"Validation Loss at Step {step}: {avg_val_loss}\")\n",
    "\n",
    "        # Save the model if validation loss improves\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            model.save_pretrained('models/summarize_model.pth')\n",
    "            tokenizer.save_pretrained('tokenizer/summarize_tokenizer')\n",
    "            print(f\"Model and tokenizer saved at Step {step} with Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "        model.train()  # Return to training mode\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 및 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드는 모델을 학습하고, validation 데이터셋으로 평가하여 최상의 모델을 저장한 뒤, test 데이터셋으로 성능을 평가하는 전체적인 프로세스를 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import random\n",
    "\n",
    "# Load the tokenizer and the model\n",
    "tokenizer = T5Tokenizer.from_pretrained('tokenizer/summarize_tokenizer')\n",
    "best_model = T5ForConditionalGeneration.from_pretrained('models/summarize_model.pth')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model.to(device)\n",
    "\n",
    "# Randomly sample 200 indices from the test dataset\n",
    "num_samples = 200\n",
    "random_indices = random.sample(range(len(test_loader.dataset)), num_samples)\n",
    "subset = Subset(test_loader.dataset, random_indices)\n",
    "sampled_test_loader = DataLoader(subset, batch_size=4, shuffle=False)\n",
    "\n",
    "best_model.eval()\n",
    "test_loss = 0.0\n",
    "\n",
    "# Store a few examples for comparison\n",
    "examples_to_show = 5\n",
    "examples = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(sampled_test_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        target_ids = batch['target_ids'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = best_model(input_ids=input_ids, labels=target_ids)\n",
    "        test_loss += outputs.loss.item()\n",
    "\n",
    "        # Decode for comparison\n",
    "        if len(examples) < examples_to_show:\n",
    "            for i in range(input_ids.size(0)):\n",
    "                if len(examples) >= examples_to_show:\n",
    "                    break\n",
    "                input_text = tokenizer.decode(input_ids[i], skip_special_tokens=True)\n",
    "                predicted_text = tokenizer.decode(\n",
    "                    best_model.generate(input_ids[i].unsqueeze(0), max_length=512)[0],\n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                target_text = tokenizer.decode(target_ids[i], skip_special_tokens=True)\n",
    "                examples.append((input_text, predicted_text, target_text))\n",
    "\n",
    "avg_test_loss = test_loss / len(sampled_test_loader)\n",
    "print(f\"Test Loss: {avg_test_loss}\")\n",
    "\n",
    "# Display examples\n",
    "print(\"\\nSample Predictions:\")\n",
    "for idx, (input_text, predicted_text, target_text) in enumerate(examples):\n",
    "    print(f\"\\nExample {idx + 1}\")\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Predicted Output: {predicted_text}\")\n",
    "    print(f\"Target Output: {target_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실제 뉴스 TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and the model\n",
    "tokenizer = T5Tokenizer.from_pretrained('/content/drive/MyDrive/tokenizer/e2e_summarize_tokenizer')  # 저장된 토크나이저 경로\n",
    "model = T5ForConditionalGeneration.from_pretrained('/content/drive/MyDrive/models/e2e_summarize_model.pth')  # 저장된 모델 경로\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to summarize an article\n",
    "def summarize_article(news):\n",
    "    # Preprocess the input text\n",
    "    input_text = f\"summarize: {news}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", truncation=True, max_length=1024).to(device)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(input_ids, max_length=300, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "\n",
    "news = '''There’s some kind of magic afoot. If, like me, you’re one of the very few people who hasn’t already seen the blockbuster stage musical Wicked (it’s the second-highest-grossing Broadway show of all time, so that’s an awful lot of bums on seats), you may approach this shiny, high-energy, relentlessly marketed movie adaptation with low to moderate expectations. There’s the unwieldy running time, for a start – two hours and 40 minutes – and the cynical, box-office-gouging decision to carve the story into two films (fans will have to wait almost a year to the day before they get to watch the concluding chapter).But here’s the thing: reservations are soon extinguished and grumbles about the release strategy swiftly quashed. Wicked matches its polished razzle-dazzle with real heart. Driven by knockout performances from Cynthia Erivo and Ariana Grande, Jon M Chu’s impossibly slick charm assault of an adaptation zips along so enjoyably that you almost wish it were longer (your bladder may disagree). With its all too timely themes of bullying, corrupt leaders and the demonisation of difference, this is a movie that promises a froth of pink and green escapism but delivers considerably more in the way of depth and darkness.For those who have somehow evaded Wicked’s cultural reach over the past couple of decades, here’s a brief primer. The film and the stage show are both loosely based on Gregory Maguire’s 1995 novel Wicked: The Life and Times of the Wicked Witch of the West, which offers an alternative backstory for the Wicked Witch from The Wizard of Oz. This film focuses on Wicked’s early years and two young witches-to-be: green-skinned outcast Elphaba (Erivo), who will go on to become the Wicked Witch of the West, and the vain, popular Galinda (Grande), who will eventually blossom into Glinda the Good.Both Elphaba and Galinda are newly arrived at Shiz University (think the student politics of Mean Girls’s North Shore High and the curriculum of Hogwarts). Although not enrolled as a student, Elphaba is on site to help her paraplegic younger sister, Nessarose (Marissa Bode). But the formidable teacher Madame Morrible (a haughtily fabulous Michelle Yeoh, breezing over any vocal limitations in a cloud of intimidating glamour) spots potential in Elphaba and offers her one-to-one tutoring in the art of enchantment. To the mutual disgust of both girls, Elphaba and Galinda find themselves assigned as roomies.Grande has a vocal range so extensive that some of it is only audible to bats, and she uses every last note of it here\n",
    "It’s not just their personalities that clash: the film’s colour palette is at first a battleground between the chlorophyll green of Elphaba’s skin and the candyfloss pink of Galinda’s wardrobe. But visuals that initially seem jarring start to find harmony as the movie progresses. A scene in a forest full of mossy tuffets of vegetation and garlands of delicate, rosy blooms is lush and lovely, one of several notable triumphs for the production design department, led by Nathan Crowley, whose credits include the similarly lavish Wonka. Likewise, Elphaba and Galinda warm to each other and a genuine connection is forged between them.\n",
    "Both lead actors impress. Erivo is terrific, her rich, velvety voice cracking under the weight of rejections and ridicule suffered by Elphaba; her eyes showing the bruises that her skin cannot. And Grande is supremely well cast. It’s not just the voice: the singer has a vocal range so extensive that some of it is only audible to bats, and she uses every last note of it here. But more crucial is her gift for physical comedy – each flouncy hair toss, each ditsy heel kick, is a precision-tooled punchline.\n",
    "Elsewhere, Bridgerton’s Jonathan Bailey, as the shallow and self-absorbed Prince Fiyero, skips away with every one of his scenes – in particular, a dizzyingly complex song-and-dance sequence in the college library. Kudos, too, to the choreographer Christopher Scott for dreaming it all up, and to cinematographer Alice Brooks for capturing the magic.\n",
    "Does it all work? There are moments that get too caught up in their own whirl of CGI pageantry and empty spectacle. And certainly some scenes could be tightened up a little – it’s worth noting that the running time of this first film instalment is longer than the stage version in its entirety. But for the most part, Wicked the movie takes flight and lifts our hearts along with it. We’re caught in the slipstream of Elphaba and her knobbly and uncomfortable-looking broomstick as she whooshes off into the second half of the story.\"\n",
    "'''\n",
    "# Generate and print the summary\n",
    "summary = summarize_article(news)\n",
    "print(\"News : \", news)\n",
    "print(\"Summary : \", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "engnews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
